{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7c21330-7326-4d98-9351-d1b2e4c6143c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create synthetic dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of classes in the dataset is set to 2 below.  Larger values for `n_classes` are also supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "291da378-0e9b-4b53-bf9e-b78c35631f1d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "n_rows = 50000\n",
    "n_cols = 300\n",
    "dtype='float32'\n",
    "X, y = make_classification(n_samples=n_rows, n_features=n_cols, n_informative=n_cols//3, \n",
    "                           n_redundant=n_cols//3, random_state=1, n_classes=n_classes)\n",
    "X = X.astype(dtype)\n",
    "y = y.astype(dtype)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9ae1eaf-a6d7-467d-88c0-644ae814c488",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Convert dataset to Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9ae1eaf-a6d7-467d-88c0-644ae814c488",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd_data_train = pd.DataFrame({\"features\": list(X_train), \"label\": y_train})\n",
    "pd_data_test = pd.DataFrame({\"features\": list(X_test), \"label\": y_test})\n",
    "df_train = spark.createDataFrame(pd_data_train)\n",
    "df_test = spark.createDataFrame(pd_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier builder\n",
    "We will use this function to build both the Spark RAPIDS ML (GPU) and Spark ML (CPU) logistic regression classifier objects, demonstrating the common API, and verify they yield similar performance on our synthetic dataset.   NOTE: GPU LogisisticRegression does not yet support `standardization=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lr_classifier(estimator_class):\n",
    "    return ( estimator_class(standardization=False)\n",
    "                .setFeaturesCol(\"features\")\n",
    "                .setLabelCol(\"label\")\n",
    "                .setRegParam(0.001)\n",
    "                .setElasticNetParam(0.5)\n",
    "                .setMaxIter(200)\n",
    "                .setTol(1.0e-30)\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90b36ddc-2f90-409b-a213-3f319c736134",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Spark RAPIDS ML (GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "559be116-df65-4d64-b549-6f1e5f813b08",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from spark_rapids_ml.classification import LogisticRegression\n",
    "gpu_lr_classifier = build_lr_classifier(LogisticRegression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5a2cac5-18cd-450a-8571-195be588a361",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Spark Rapids ML estimator can be persisted and reloaded similarly to Spark ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_path = \"/tmp/spark-rapids-ml-lr-classifier-estimator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "798f5b5b-cfa6-45e4-aa18-40c0142e894a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gpu_lr_classifier.write().overwrite().save(estimator_path)\n",
    "gpu_lr_classifier_loaded = LogisticRegression.load(estimator_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "209d8f9c-1d1c-4bdd-880c-10e46f9d0c49",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "gpu_model = gpu_lr_classifier_loaded.fit(df_train)\n",
    "print(f\"Fit took: {time.time() - start_time} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db84a983-a64f-462b-813b-7b7ba629d18e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gpu_model.coefficients[0:10] if gpu_model.numClasses <= 2 else gpu_model.coefficientMatrix.toArray()[:,0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_model.numClasses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/tmp/spark-rapids-ml-lr-classifier-model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6874b600-707d-48a7-b780-5d4e939a746b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gpu_model.write().overwrite().save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "441c8635-9070-426f-8626-7083f34b7e71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gpu_model_loaded = gpu_model.read().load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48f7975c-f2ce-471d-aa8e-8dc678a44f37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gpu_model_loaded.coefficients[0:10] if gpu_model_loaded.numClasses <= 2 else gpu_model_loaded.coefficientMatrix.toArray()[:,0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_model_loaded.numClasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3bfc179-e702-4198-8122-3a8ba98113a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "transformed_df = gpu_model_loaded.setPredictionCol(\"prediction\").setProbabilityCol(\"probability\").transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de0ce32d-3d09-4e82-b6a7-26978925181a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "transformed_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c63b707-0e00-4961-8a76-82f347886b83",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "transformed_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7c5d3b5-ecb0-435a-8976-bc18a28e3e04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "transformed_df.select(\"features\",\"label\",\"prediction\",\"probability\").sort(\"features\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the auc on the test set of the GPU trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = ( BinaryClassificationEvaluator() \n",
    "                .setRawPredictionCol(\"probability\")\n",
    "                .setLabelCol(\"label\")\n",
    "            ) if gpu_model_loaded.numClasses <= 2 else (\n",
    "                MulticlassClassificationEvaluator()\n",
    "                .setProbabilityCol(\"probability\")\n",
    "                .setLabelCol(\"label\")\n",
    "                .setMetricName(\"logLoss\")\n",
    "            )\n",
    "if gpu_model_loaded.numClasses <= 2:\n",
    "    print(f\"auc: {evaluator.evaluate(transformed_df)}\")\n",
    "else:\n",
    "    print(f\"logLoss: {evaluator.evaluate(transformed_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ccc646a3-6063-42f4-8909-03c285ce55d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Spark ML (CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "122a7ab9-d142-4244-b79c-bbf7e6eb05e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "cpu_lr_classifier = build_lr_classifier(LogisticRegression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd173dd2-7058-4d73-a969-c20e9f55e3e4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Convert array sql type to VectorUDT expected by Spark ML (Note: Spark RAPIDS ML also accepts VectorUDT Dataframes in addition to array type Dataframe above, along with a scalar column format - see docs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f8a18da-17a5-4fcc-b2a5-604ea1088e1b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.functions import array_to_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9882dae-8c66-4e4d-8164-36040e2f0f3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "vector_df_train = df_train.select(array_to_vector(df_train.features).alias(\"features\"),\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a48fdc86-e827-4545-a923-109519c675d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "cpu_model = cpu_lr_classifier.fit(vector_df_train)\n",
    "print(f\"Fit took: {time.time() - start_time} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fb9b7ea-15db-4678-8087-40b69c13c7bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cpu_model.coefficients[0:10] if cpu_model.numClasses <= 2 else cpu_model.coefficientMatrix.toArray()[:,0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_model.numClasses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_df_test = df_test.select(array_to_vector(df_test.features).alias(\"features\"),\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec2a3245-fa44-4b08-b4f0-dc9c16b1528e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cpu_transformed_df = cpu_model.setPredictionCol(\"prediction\").setProbabilityCol(\"probability\").transform(vector_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c02f6f1-36ac-4a02-995f-6ad68ee38cec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cpu_transformed_df.select(\"features\",\"label\",\"prediction\",\"probability\").sort(\"features\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test set AUCs of GPU model above and CPU model below are comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if cpu_model.numClasses <= 2:\n",
    "    print(f\"auc: {evaluator.evaluate(cpu_transformed_df)}\")\n",
    "else:\n",
    "    print(f\"logLoss: {evaluator.evaluate(cpu_transformed_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization needs to be false for now. Will be fixed in 24.02."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataframe\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer, RegexTokenizer\n",
    "from pyspark.sql import Row\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "try:\n",
    "    twenty_train = fetch_20newsgroups(subset=\"train\", shuffle=True, random_state=42)\n",
    "except:\n",
    "    print(\"Error fetching 20 newsgroup dataset\")\n",
    "X = twenty_train.data\n",
    "y = twenty_train.target.tolist()\n",
    "\n",
    "data = [\n",
    "    Row(\n",
    "        label=y[i],\n",
    "        weight=1.0,\n",
    "        text=X[i],\n",
    "    )\n",
    "    for i in range(len(X))\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "# convert text to sparse vector\n",
    "tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
    "df = tokenizer.transform(df)\n",
    "cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"features\")\n",
    "cv_model = cv.fit(df)\n",
    "df = cv_model.transform(df)\n",
    "\n",
    "df_train, df_test = df.randomSplit([0.8, 0.2], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_vectors_compat(EstimatorClass):\n",
    "    from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "    lr = EstimatorClass(\n",
    "        regParam=0.01,\n",
    "        maxIter=100,\n",
    "        fitIntercept=True,\n",
    "        standardization=False,\n",
    "        featuresCol=\"features\",\n",
    "        labelCol=\"label\",\n",
    "    )\n",
    "\n",
    "    # fit and transform\n",
    "    start_time = time.time()\n",
    "    model = lr.fit(df_train)\n",
    "    fit_time = time.time() - start_time\n",
    "\n",
    "    trainsformed_df_test = model.transform(df_test)\n",
    "\n",
    "    # evaluate\n",
    "    evaluator = (\n",
    "        MulticlassClassificationEvaluator()\n",
    "        .setPredictionCol(model.getPredictionCol())\n",
    "        .setProbabilityCol(model.getProbabilityCol())\n",
    "        .setLabelCol(model.getLabelCol())\n",
    "    )\n",
    "    \n",
    "    evaluator.setMetricName(\"logLoss\")\n",
    "    test_logLoss = evaluator.evaluate(trainsformed_df_test)\n",
    "    return (lr, model, fit_time, test_logLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spark_rapids_ml.classification import LogisticRegression as GPULR\n",
    "gpu_lr, gpu_model, gpu_fit_time, gpu_test_logLoss = sparse_vectors_compat(GPULR)\n",
    "print(f\"GPU fit took: {gpu_fit_time} sec\")\n",
    "print(f\"GPU training objective: {gpu_model.objective}\")\n",
    "print(f\"GPU test logLoss: {gpu_test_logLoss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression as CPULR\n",
    "cpu_lr, cpu_model, cpu_fit_time, cpu_test_logLoss = sparse_vectors_compat(CPULR)\n",
    "print(f\"CPU fit took: {cpu_fit_time} sec\")\n",
    "print(f\"CPU training objective: {cpu_model.summary.objectiveHistory[-1]}\")\n",
    "print(f\"CPU test logLoss: {cpu_test_logLoss}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "spark-rapids-ml-lr-demo",
   "notebookOrigID": 1026070411409745,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
