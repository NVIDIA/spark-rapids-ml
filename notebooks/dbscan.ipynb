{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7c21330-7326-4d98-9351-d1b2e4c6143c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "291da378-0e9b-4b53-bf9e-b78c35631f1d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "n_rows = 10000\n",
    "n_cols = 500\n",
    "n_clusters_data = 10\n",
    "cluster_std = 1.0\n",
    "dtype='float32'\n",
    "from sklearn.datasets import make_blobs\n",
    "data, _ = make_blobs(\n",
    "        n_samples=n_rows, n_features=n_cols, centers=n_clusters_data, cluster_std=cluster_std, random_state=0\n",
    "    )  # make_blobs creates a random dataset of isotropic gaussian blobs.\n",
    "\n",
    "data = data.astype(dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9ae1eaf-a6d7-467d-88c0-644ae814c488",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Convert dataset to Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9ae1eaf-a6d7-467d-88c0-644ae814c488",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd_data = pd.DataFrame({\"features\": list(data)})\n",
    "df = spark.createDataFrame(pd_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90b36ddc-2f90-409b-a213-3f319c736134",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Spark RAPIDS ML DBSCAN (GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1f994f0-4ca6-4b63-88f7-b0ac94ee3130",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from spark_rapids_ml.clustering import DBSCAN\n",
    "gpu_dbscan = DBSCAN(eps=50.0, min_samples=3).setFeaturesCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5a2cac5-18cd-450a-8571-195be588a361",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Estimator can be persisted and reloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_path = \"/tmp/dbscan-estimator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "798f5b5b-cfa6-45e4-aa18-40c0142e894a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gpu_dbscan.write().overwrite().save(estimator_path)\n",
    "gpu_dbscan_loaded = DBSCAN.load(estimator_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "209d8f9c-1d1c-4bdd-880c-10e46f9d0c49",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "gpu_model = gpu_dbscan_loaded.fit(df)\n",
    "print(f\"Fit took: {time.time() - start_time} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77499eff-5cf2-4ce6-95a1-e45b69abe3cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gpu_dbscan_loaded.getEps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/tmp/dbscan-model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6874b600-707d-48a7-b780-5d4e939a746b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gpu_model.write().overwrite().save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "441c8635-9070-426f-8626-7083f34b7e71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gpu_model_loaded = gpu_model.read().load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3bfc179-e702-4198-8122-3a8ba98113a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "transformed_df = gpu_model_loaded.setPredictionCol(\"transformed\").transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de0ce32d-3d09-4e82-b6a7-26978925181a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "transformed_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c63b707-0e00-4961-8a76-82f347886b83",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "transformed_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7c5d3b5-ecb0-435a-8976-bc18a28e3e04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "transformed_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare DBSCAN vs KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Ring Shape Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_points_in_ring(center, inner_radius, outer_radius, num_points):\n",
    "    # Generate random angles\n",
    "    angles = np.random.uniform(0, 2 * np.pi, num_points)\n",
    "\n",
    "    # Generate random radii within the ring\n",
    "    radii = np.sqrt(np.random.uniform(inner_radius**2, outer_radius**2, num_points))\n",
    "\n",
    "    # Convert polar coordinates to Cartesian coordinates\n",
    "    x = center[0] + radii * np.cos(angles)\n",
    "    y = center[1] + radii * np.sin(angles)\n",
    "\n",
    "    # Create array of points\n",
    "    points = np.column_stack((x, y))\n",
    "\n",
    "    return points\n",
    "\n",
    "data_inner = generate_random_points_in_ring((0,0), 1, 2, 500)\n",
    "data_outer = generate_random_points_in_ring((0,0), 4, 5, 500)\n",
    "data = np.concatenate((data_inner, data_outer), axis=0)\n",
    "np.random.shuffle(data)\n",
    "\n",
    "pd_data = pd.DataFrame({\"features\": list(data)})\n",
    "df = spark.createDataFrame(pd_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=1.0, min_samples=5).setFeaturesCol(\"features\")\n",
    "dbscan_model = dbscan.fit(df)\n",
    "dbscan_transformed = dbscan_model.transform(df)\n",
    "\n",
    "dbscan_pd = dbscan_transformed.toPandas()\n",
    "dbscan_np = dbscan_pd.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spark_rapids_ml.clustering import KMeans\n",
    "kmeans =  KMeans(k=2).setFeaturesCol(\"features\")\n",
    "kmeans_model = kmeans.fit(df)\n",
    "kmeans_transformed = kmeans_model.transform(df)\n",
    "\n",
    "kmeans_pd = kmeans_transformed.toPandas()\n",
    "kmeans_np = kmeans_pd.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Clustering Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cluster0 = []\n",
    "cluster1 = []\n",
    "for p in kmeans_np:\n",
    "    if (p[1] == 0):\n",
    "        cluster0.append(p[0])\n",
    "    else:\n",
    "        cluster1.append(p[0])\n",
    "\n",
    "cluster0 = np.array(cluster0)\n",
    "cluster1 = np.array(cluster1)\n",
    "        \n",
    "plt.scatter(cluster0[:, 0], cluster0[:, 1], s=5, label=\"cluster 0\")\n",
    "plt.scatter(cluster1[:, 0], cluster1[:, 1], s=5, label=\"cluster 1\")\n",
    "    \n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('KMeans Clustering Result')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster0 = []\n",
    "cluster1 = []\n",
    "for p in dbscan_np:\n",
    "    if (p[1] == 0):\n",
    "        cluster0.append(p[0])\n",
    "    else:\n",
    "        cluster1.append(p[0])\n",
    "\n",
    "cluster0 = np.array(cluster0)\n",
    "cluster1 = np.array(cluster1)\n",
    "        \n",
    "plt.scatter(cluster0[:, 0], cluster0[:, 1], s=5, label=\"cluster 0\")\n",
    "plt.scatter(cluster1[:, 0], cluster1[:, 1], s=5, label=\"cluster 1\")\n",
    "    \n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('DBSCAN Clustering Result')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data and Store to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full dataset\n",
    "# !curl --output twitter.h5.h5 https://b2share.eudat.eu/api/files/189c8eaf-d596-462b-8a07-93b5922c4a9f/twitter.h5.h5\n",
    "\n",
    "# Partial small dataset\n",
    "!curl --output twitterSmall.h5.h5 https://b2share.eudat.eu/api/files/189c8eaf-d596-462b-8a07-93b5922c4a9f/twitterSmall.h5.h5\n",
    "\n",
    "import h5py\n",
    "import pyarrow\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "with h5py.File('twitterSmall.h5.h5', 'r') as f: \n",
    "    data = f[\"DBSCAN\"][:]\n",
    "\n",
    "df=pd.DataFrame(data, columns=['f1', 'f2'])\n",
    "arrow_table = pyarrow.Table.from_pandas(df)\n",
    "\n",
    "# REMEMBER to change the dbfs path to your designated space\n",
    "#   Or to local like \"./twitter.parquet\"\n",
    "dbfs_path = \"/dbfs/temp/twitter.parquet\"\n",
    "pq.write_table(arrow_table, dbfs_path)\n",
    "\n",
    "df = spark.read.parquet(dbfs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run DBSCAN over Twitter Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "eps = 0.1\n",
    "gpu_dbscan = DBSCAN(eps=eps, min_samples=40, metric=\"euclidean\")\n",
    "gpu_dbscan.setFeaturesCols([\"f1\", \"f2\"])\n",
    "gpu_model = gpu_dbscan.fit(df)\n",
    "gpu_model.setPredictionCol(\"prediction\")\n",
    "transformed = gpu_model.transform(df)\n",
    "transformed.show()\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = (end_time - start_time)\n",
    "\n",
    "print(\"Time\", elapsed_time)\n",
    "\n",
    "dbscan_np = transformed.toPandas().to_numpy()\n",
    "\n",
    "n_cluster = max(dbscan_np[:,2])\n",
    "clusters = [[[],[]] for i in range(int(n_cluster) + 1)]\n",
    "\n",
    "for p in dbscan_np:\n",
    "    if int(p[2]) == -1:\n",
    "        continue\n",
    "\n",
    "    clusters[int(p[2])][0].append(p[0])\n",
    "    clusters[int(p[2])][1].append(p[1])\n",
    "\n",
    "clusters = sorted(clusters, key=lambda x: len(x[0]), reverse=True)\n",
    "print(\"Number of clusters: \", len(clusters))\n",
    "\n",
    "for i, c in enumerate(clusters):\n",
    "    plt.scatter(c[0], c[1], s=0.5, label=f\"cluster {i}\")\n",
    "    \n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title(f'Twitter API Geo Clusters with DBSCAN eps={eps}')\n",
    "plt.show()\n",
    "# plt.savefig('plot.png', dpi=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "spark-rapids-ml-kmeans-demo",
   "notebookOrigID": 1026070411409745,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
